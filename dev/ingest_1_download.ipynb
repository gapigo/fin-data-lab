{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c97cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from common.postgresql import PostgresConnector\n",
    "db = PostgresConnector()\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# Configurações\n",
    "BASE_URL = \"https://dados.cvm.gov.br/dados/\"\n",
    "DEST_DIR = r\"E:/Download/cvm\"\n",
    "\n",
    "# Inicializa banco de dados\n",
    "connector = db\n",
    "\n",
    "def setup_control_table():\n",
    "    \"\"\"Garante que a tabela de controle exista antes de iniciar.\"\"\"\n",
    "    # Cria um DF vazio com a estrutura necessária para o controle\n",
    "    df_schema = pd.DataFrame(columns=['file_name', 'relative_path', 'last_download'])\n",
    "    # create_table está blindada: se a tabela existir, ela não faz nada\n",
    "    connector.create_table(df_schema, 'cvm.ingest_control')\n",
    "\n",
    "def check_should_skip(file_name, relative_path):\n",
    "    \"\"\"\n",
    "    Verifica se deve pular o arquivo.\n",
    "    Prioridade 1: Banco de dados (últimos 7 dias)\n",
    "    Prioridade 2: Existência do arquivo físico\n",
    "    \"\"\"\n",
    "    # 1. Verifica no Banco de Dados\n",
    "    # O query busca se o arquivo foi baixado nos últimos 7 dias\n",
    "    query = f\"\"\"\n",
    "        SELECT 1 FROM cvm.ingest_control \n",
    "        WHERE file_name = '{file_name}' \n",
    "        AND relative_path = '{relative_path}'\n",
    "        AND last_download > CURRENT_DATE - INTERVAL '7 days'\n",
    "    \"\"\"\n",
    "    res = connector.read_sql(query)\n",
    "    if not res.empty:\n",
    "        return True\n",
    "\n",
    "    # 2. Verifica na Pasta Destino\n",
    "    # Se o CSV correspondente já existe, pula também\n",
    "    local_csv_path = os.path.join(DEST_DIR, relative_path).replace('.zip', '.csv')\n",
    "    if os.path.exists(local_csv_path):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def register_download(file_name, relative_path):\n",
    "    \"\"\"Registra o sucesso do download na tabela de controle.\"\"\"\n",
    "    df_log = pd.DataFrame([{\n",
    "        'file_name': file_name,\n",
    "        'relative_path': relative_path,\n",
    "        'last_download': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }])\n",
    "    # Upsert garante que se o arquivo for baixado novamente após 7 dias, a data seja atualizada\n",
    "    connector.upsert_dataframe(df_log, 'cvm.ingest_control', logical_pks=['file_name', 'relative_path'])\n",
    "\n",
    "# Conjunto para rastrear o que já foi mapeado e evitar loops\n",
    "visited_urls = set()\n",
    "\n",
    "def is_valid_url(url):\n",
    "    parsed = urlparse(url)\n",
    "    return parsed.netloc == \"dados.cvm.gov.br\" and parsed.path.startswith(\"/dados/\")\n",
    "\n",
    "def get_content(url):\n",
    "    try:\n",
    "        if url in visited_urls or not is_valid_url(url):\n",
    "            return [], []\n",
    "        \n",
    "        visited_urls.add(url)\n",
    "        response = requests.get(url, timeout=15)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        links = [node.get('href') for node in soup.find_all('a') if node.get('href')]\n",
    "        pastas, arquivos = [], []\n",
    "        \n",
    "        for link in links:\n",
    "            if link in ['../', './', '/'] or link.startswith('?') or link.startswith('http'):\n",
    "                if not link.startswith('http'): pass \n",
    "                else:\n",
    "                    if not is_valid_url(link): continue\n",
    "\n",
    "            full_url = urljoin(url, link)\n",
    "            if link.endswith('/'):\n",
    "                if full_url not in visited_urls:\n",
    "                    pastas.append(full_url)\n",
    "            elif link.lower().endswith(('.zip', '.csv')):\n",
    "                arquivos.append(full_url)\n",
    "                \n",
    "        return pastas, arquivos\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao acessar {url}: {e}\")\n",
    "        return [], []\n",
    "\n",
    "def download_and_unzip(url):\n",
    "    relative_path = urlparse(url).path.replace(\"/dados/\", \"\").lstrip(\"/\")\n",
    "    file_name = url.split('/')[-1]\n",
    "    \n",
    "    # VERIFICAÇÃO DE PULO\n",
    "    if check_should_skip(file_name, relative_path):\n",
    "        # Omiti o print para não poluir o terminal, mas o crawler continua\n",
    "        return\n",
    "\n",
    "    sub_folders = os.path.split(relative_path)[0]\n",
    "    local_dir = os.path.join(DEST_DIR, sub_folders)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    file_path = os.path.join(local_dir, file_name)\n",
    "\n",
    "    print(f\"Processando: {relative_path}\")\n",
    "    try:\n",
    "        r = requests.get(url, stream=True)\n",
    "        with open(file_path, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=1024*1024):\n",
    "                f.write(chunk)\n",
    "        \n",
    "        if file_name.lower().endswith('.zip'):\n",
    "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(local_dir)\n",
    "            os.remove(file_path)\n",
    "        \n",
    "        # REGISTRO NO BANCO APÓS SUCESSO\n",
    "        register_download(file_name, relative_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Falha em {file_name}: {e}\")\n",
    "\n",
    "def run_crawler(start_url):\n",
    "    # Passo 0: Garantir tabela de controle\n",
    "    setup_control_table()\n",
    "    \n",
    "    stack = [start_url]\n",
    "    while stack:\n",
    "        current_url = stack.pop()\n",
    "        sub_pastas, arquivos = get_content(current_url)\n",
    "        \n",
    "        for arq in arquivos:\n",
    "            download_and_unzip(arq)\n",
    "            \n",
    "        for pasta in sub_pastas:\n",
    "            stack.append(pasta)\n",
    "\n",
    "print(f\"Iniciando Crawling em {BASE_URL}...\")\n",
    "run_crawler(BASE_URL)\n",
    "print(\"\\nFinalizado!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
